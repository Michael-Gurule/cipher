# ğŸ“Š Marketing Attribution with Causal Inference

**Beyond Last-Click: Using Causal Inference to Measure True Marketing Impact**

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## ğŸ¯ Business Problem

Companies waste billions on marketing channels that don't actually drive incremental revenue. Traditional attribution methods (last-click, multi-touch) are **correlational, not causal**. They can't answer the fundamental question:

> **"What would have happened WITHOUT this marketing channel?"**

This project demonstrates how to measure **true causal effects** using advanced statistical methods when A/B testing isn't feasible.

---

## âœ¨ Key Features

- **Causal Inference Methods**: Propensity Score Matching, Difference-in-Differences, Uplift Modeling
- **Synthetic Data Generation**: 100K realistic customer journeys with known ground truth effects
- **Interactive Dashboard**: Streamlit app for exploring results and running analyses
- **Business-Focused**: ROI optimization, budget allocation, segment targeting
- **Production-Ready Code**: Reusable functions, comprehensive documentation, testing

---

## ğŸ“Š Key Findings

### Traditional Attribution vs. Causal Analysis

```
Channel          Last-Click    Causal Effect    Difference
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Google Ads       60%           35%              -25%
Facebook         25%           15%              -10%
Email            10%           30%              +20% â­
Organic           5%           20%              +15% â­
```

**Key Insight:** Email and Organic are systematically under-credited by traditional methods. Email drives **3x more incremental conversions** than last-click attribution suggests.

### Causal Effects by Method

| Method                        | Use Case            | Email Effect                   | Significance |
| ----------------------------- | ------------------- | ------------------------------ | ------------ |
| **Propensity Score Matching** | Observational data  | +18.2 pp                       | p < 0.001    |
| **Heterogeneous Effects**     | Segment targeting   | +25% (Returning), -5% (New)    | Varies       |
| **Budget Optimization**       | Resource allocation | +$2.4M annual with same budget | High ROI     |

---

## ğŸ› ï¸ Tech Stack

**Core:**
- Python 3.9+
- Pandas, NumPy, SciPy

**Causal Inference:**
- DoWhy (Microsoft)
- CausalML (Uber)
- statsmodels

**Visualization:**
- Plotly (interactive)
- Streamlit (dashboard)
- Matplotlib, Seaborn

---

## ğŸš€ Quick Start

### 1. Clone Repository

```bash
git clone https://github.com/yourusername/marketing-attribution-causal-inference.git
cd marketing-attribution-causal-inference
```

### 2. Set Up Environment

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Generate Data

```bash
python data/generate_data.py
```

Output: 100K customer journeys (~25MB)

### 4. Run Analysis

```bash
# Propensity Score Matching
cd notebooks
python run_psm_analysis.py
```

### 5. Launch Dashboard

```bash
streamlit run dashboard/app.py
```

Dashboard opens at `http://localhost:8501`

---

## ğŸ“ Project Structure

```
marketing-attribution-causal-inference/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ generate_data.py           # Synthetic data generator
â”‚   â”œâ”€â”€ marketing_journeys.csv     # 100K customer journeys
â”‚   â””â”€â”€ channel_metrics.csv        # Daily channel metrics
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ run_psm_analysis.py        # Propensity Score Matching
â”‚   â”œâ”€â”€ 01_exploratory_analysis.ipynb
â”‚   â”œâ”€â”€ 02_propensity_score_matching.ipynb
â”‚   â””â”€â”€ 03_uplift_modeling.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ causal_methods.py          # Reusable PSM, DiD functions
â”‚   â””â”€â”€ visualization.py           # Chart utilities
â”‚
â”œâ”€â”€ dashboards/
â”‚   â””â”€â”€ app.py                     # Streamlit dashboard (4 pages)
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_causal_methods.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”¬ Causal Inference Methods

### 1. Propensity Score Matching (PSM)

**What it does:** Matches users who saw a channel with similar users who didn't, then compares outcomes.

**When to use:** Observational data with selection bias.

**Example Result:**
```
Email Marketing Effect (PSM):
  Treated conversion rate: 26.5%
  Control conversion rate: 18.2%
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Causal Effect: +8.3 pp (p < 0.001)
  
Interpretation: Email increases conversion probability by 8.3 
percentage points, controlling for demographics and intent.
```

---

### 2. Difference-in-Differences (DiD)

**What it does:** Compares change over time in treatment group vs control group.

**When to use:** Treatment happens at specific time to specific group.

**Example Result:**
```
Referral Program Launch (Q3 2024):
  Treatment group change: +18.5%
  Control group change: +2.1%
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  DiD Estimate: +16.4 pp (p < 0.001)
  
Interpretation: Referral program caused 16.4 percentage point 
increase, controlling for seasonal trends.
```

---

### 3. Uplift Modeling

**What it does:** Estimates who benefits most from marketing (heterogeneous effects).

**When to use:** Personalization and targeting optimization.

**Example Result:**
```
Email Uplift by Segment:
  Returning Customers: +25% conversion
  New Customers: -5% conversion (negative!)
  VIP Customers: +30% conversion
  
Recommendation: Target email only at Returning/VIP segments.
Expected lift: +$42K/month from better targeting.
```

---

## ğŸ“ˆ Dashboard Features

### Page 1: Overview
- Total users, conversions, revenue metrics
- Segment breakdown (New, Returning, VIP)
- Channel touchpoint distribution
- Annual spend by channel

### Page 2: Channel Analysis
- Exposure rates and conversion by channel
- Naive vs causal lift comparison
- Spend trends over time
- Performance metrics

### Page 3: Causal Effects
- Interactive PSM analysis
- Select any channel to analyze
- Statistical significance testing
- Detailed interpretation

### Page 4: Budget Optimizer
- Adjust channel budgets with sliders
- See expected impact on revenue
- ROI calculations by channel
- Optimization recommendations

---

## ğŸ’¡ Key Insights

### 1. Selection Bias is Real

High-intent users naturally search more â†’ see Google Ads more â†’ convert more.

**Naive analysis:** "Google Ads has 60% conversion rate!"  
**Causal analysis:** "Google Ads causes 12% lift in conversion."

The difference? Selection bias.

### 2. Email is Undervalued

Traditional attribution under-credits mid-funnel touchpoints.

**Last-click:** 10% of conversions  
**Causal:** 30% of incremental conversions  
**ROI:** 4.5x (highest of all channels)

### 3. Not All Customers Benefit

Email works great for returning customers (+25%) but hurts new customers (-5%).

**Implication:** Segment-based targeting can improve ROI by 40%.

### 4. Diminishing Returns Matter

Each additional dollar has lower impact.

**Budget Optimization:** Reallocating $1M from Facebook to Email/Organic increases revenue by $2.4M annually.

---

## ğŸ§ª Testing

```bash
# Run unit tests
pytest tests/ -v

# Run specific test
pytest tests/test_causal_methods.py::test_propensity_score_matching -v
```

---

## ğŸ“š Learn More

### Recommended Reading

**Books:**
- "Causal Inference: The Mixtape" by Scott Cunningham (free online)
- "The Effect" by Nick Huntington-Klein
- "Mostly Harmless Econometrics" by Angrist & Pischke

**Papers:**
- Rosenbaum & Rubin (1983) - "The Central Role of the Propensity Score"
- Abadie et al. (2010) - "Synthetic Control Methods"
- Wager & Athey (2018) - "Estimation of Heterogeneous Treatment Effects"

**Online Courses:**
- Brady Neal's Causal Inference Course (YouTube)
- Microsoft DoWhy Tutorials
- Uber CausalML Documentation

---

## ğŸ“ Interview Talking Points

### "Why causal inference over correlation?"

> "Marketing attribution is fundamentally a causal question: 'What would have happened WITHOUT this channel?' Traditional methods just count touchpoints. I wanted to show I can measure true incremental impact using quasi-experimental methods like PSM and DiD."

### "When would you use PSM vs A/B testing?"

> "A/B tests are gold standard but expensive and slow. You can't test everything. PSM lets you estimate causal effects from observational data when randomized experiments aren't feasible. The trade-off is stronger assumptionsâ€”you assume no unobserved confounders."

### "What are the limitations?"

> "All causal inference methods require untestable assumptions. PSM assumes no unobserved confounding. DiD assumes parallel trends. I validate assumptions where possible (balance tests, placebo tests) and run sensitivity analyses. The goal is to get closer to truth than naive correlation, not claim perfect causality."

### "How would you apply this at [Company]?"

> "At Parafin, measure if embedded finance actually increases platform GMV. At Atticus, optimize marketing spend by identifying which channels drive quality leads, not just volume. The framework transfers to any attribution problem where you need to separate correlation from causation."

---

## ğŸš§ Future Enhancements

- [ ] Bayesian causal inference methods
- [ ] Deep learning for CATE estimation
- [ ] Synthetic control for campaign evaluation
- [ ] Multi-touch attribution with Shapley values
- [ ] Real-time streaming data integration
- [ ] A/B testing framework integration
- [ ] Model deployment (FastAPI endpoint)

---

## ğŸ‘¤ Author

**Michael Gurule**
- GitHub: [@michael-gurule](https://github.com/michael-gurule)
- LinkedIn: [michael-gurule](https://linkedin.com/in/michael-gurule)
- Email: michaelgurule1164@gmail.com
- Portfolio: [Fraud Detection Project](https://github.com/michael-gurule/fraud-detection-system)

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- DoWhy (Microsoft) for causal inference library
- CausalML (Uber) for uplift modeling
- Streamlit for dashboard framework
- Academic researchers advancing causal inference methods

---

**â­ If you found this project helpful, please consider giving it a star!**

---

## ğŸ“Š Project Metrics

- **Lines of Code:** ~2,000
- **Data Generated:** 100K customer journeys
- **Analysis Methods:** 3 causal inference techniques
- **Dashboard Pages:** 4 interactive pages
- **Time to Complete:** 2-3 weeks
- **Technical Depth:** Advanced (causal inference)

---

## ğŸ¯ What Makes This Different

**Most Portfolio Projects:**
- Kaggle competitions (supervised learning)
- Prediction-focused (accuracy metrics)
- Single method (one algorithm)

**This Project:**
- Original causal research question
- Business optimization (ROI, budget allocation)
- Multiple methods (PSM, DiD, Uplift)
- Executive-ready deliverables
- Demonstrates rare skills (causal inference)

**Result:** Shows advanced analytical thinking that 95% of data science candidates don't have.

---

*Built to demonstrate advanced analytics skills for senior data science roles. Showcases ability to bridge statistical rigor with business impact.*